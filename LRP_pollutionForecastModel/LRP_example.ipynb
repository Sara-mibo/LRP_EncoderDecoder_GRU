{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: WESE, 1: RODE, 2: SOES, 3: MSGE, 4: WALS, 5: HUE2, 6: LOER, 7: DMD2, 8: WULA, 9: STYR, 10: EIFE, 11: LEV2, 12: SOLI, 13: BOTT, 14: SHW2, 15: BORG, 16: CHOR, 17: ROTH, 18: NIED, 19: AABU, 20: RAT2, 21: BIEL, "
     ]
    }
   ],
   "source": [
    "import utils_data as ud\n",
    "\n",
    "x,xf,tr,st= ud.load_data('data/x_xf_tr_st_NO2.p',device)\n",
    "cfg, mean, std, station_mean, station_std= ud.load_configurations()\n",
    "min_data,max_data= ud.load_minmax()\n",
    "##print station numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select hour of prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 168, 71])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour=5\n",
    "inp_p = x[[hour],:,:]   \n",
    "inp_f = xf[[hour],:,:]   \n",
    "targs = tr[[hour],:,:]   \n",
    "stat=st[[hour],:]   \n",
    "inp_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../LRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import utils_model as um\n",
    "\n",
    "loading_path='model/no_binarystd_GRUh1024l2dr0.3O3v0/last_checkpoint'\n",
    "wi_tuples,wh_tuples,bi_tuples,bh_tuples,wl,bl= um.model_parameters(loading_path,device,cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity decoder\n",
      " 7.8141885638664732028 in comparison with 7.81418856386647298 \n",
      "sanityCheckResult True\n",
      "sanity check encoder\n",
      " 7.8141885638664732028 in comparison with 7.8141885638664732162 \n",
      "sanityCheckResult True\n"
     ]
    }
   ],
   "source": [
    "import lrp_function as lrp\n",
    "\n",
    "mask=(hour,2) ## mask=(hour,pollutant) where pollutant is selected from ['NO', 'PM10', 'NO2','O3']\n",
    "len_static=stat.shape[-1]\n",
    "lrp_seq2seq= lrp.lrp_decoder_encoder(cfg,len_static, wi_tuples,wh_tuples,bi_tuples,bh_tuples,\n",
    "                                          wl,bl)\n",
    "\n",
    "## y: output of forward pass LRP\n",
    "## Rxf: relevance scores of input sequence forecast\n",
    "## Rx : relevance scores of input sequence historical\n",
    "## Ry: relevance scores of intermidiate decoder outputs\n",
    "## Rstatic: relevance scores of static input (satation number) \n",
    "y,Rxf,Rx,Ry,Rstatic=lrp_seq2seq.lrp_seq2seq(inp_p,inp_f,stat,mask,epsilon=0.01,bias_factor=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare LRP forward pass result  with predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=um.load_model(cfg,device,loading_path)\n",
    "prediction= model.forward(inp_p,inp_f,stat)\n",
    "\n",
    "torch.allclose(prediction,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

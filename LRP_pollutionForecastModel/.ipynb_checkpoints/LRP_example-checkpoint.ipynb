{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: WESE, 1: RODE, 2: SOES, 3: MSGE, 4: WALS, 5: HUE2, 6: LOER, 7: DMD2, 8: WULA, 9: STYR, 10: EIFE, 11: LEV2, 12: SOLI, 13: BOTT, 14: SHW2, 15: BORG, 16: CHOR, 17: ROTH, 18: NIED, 19: AABU, 20: RAT2, 21: BIEL, "
     ]
    }
   ],
   "source": [
    "import utils_data as ud\n",
    "\n",
    "x,xf,tr,st= ud.load_data('data/x_xf_tr_st_NO2.p',device)\n",
    "cfg, mean, std, station_mean, station_std= ud.load_configurations()\n",
    "min_data,max_data= ud.load_minmax()\n",
    "##print station numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select hour of prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 168, 71])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour=5\n",
    "inp_p = x[[hour],:,:]   \n",
    "inp_f = xf[[hour],:,:]   \n",
    "targs = tr[[hour],:,:]   \n",
    "stat=st[[hour],:]   \n",
    "inp_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../LRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import utils_model as um\n",
    "\n",
    "loading_path='model/no_binarystd_GRUh1024l2dr0.3O3v0/last_checkpoint'\n",
    "wi_tuples,wh_tuples,bi_tuples,bh_tuples,wl,bl= um.model_parameters(loading_path,device,cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity decoder\n",
      " 7.8141885638664767555 in comparison with 7.81418856386647442 \n",
      "sanityCheckResult True\n",
      "sanity check encoder\n",
      " 7.8141885638664767555 in comparison with 7.8141885638664748638 \n",
      "sanityCheckResult True\n"
     ]
    }
   ],
   "source": [
    "import lrp_function as lrp\n",
    "\n",
    "mask=(hour,2) ## mask=(hour,pollutant) where pollutant is selected from ['NO', 'PM10', 'NO2','O3']\n",
    "len_static=stat.shape[-1]\n",
    "lrp_seq2seq= lrp.lrp_decoder_encoder(cfg,len_static, wi_tuples,wh_tuples,bi_tuples,bh_tuples,\n",
    "                                          wl,bl)\n",
    "\n",
    "## y: output of forward pass LRP\n",
    "## Rxf: relevance scores of input sequence forecast\n",
    "## Rx : relevance scores of input sequence historical\n",
    "## Ry: relevance scores of intermidiate decoder outputs\n",
    "## Rstatic: relevance scores of static input (satation number) \n",
    "y,Rxf,Rx,Ry,Rstatic=lrp_seq2seq.lrp_seq2seq(inp_p,inp_f,stat,mask,epsilon=0.01,bias_factor=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare LRP forward pass result  with predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/no_binarystd_GRUh1024l2dr0.3O3v0/last_checkpoint\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu device type at start of device string: model/no_binarystd_GRUh1024l2dr0.3O3v0/last_checkpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-52ae2e27ba0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloading_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LRP/lrp-project/LRP_pollutionForecastModel/../LRP/utils_model.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(cfg, device, loading_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m                            dec_dropout=0, device=device, arch=cfg.arch)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu device type at start of device string: model/no_binarystd_GRUh1024l2dr0.3O3v0/last_checkpoint"
     ]
    }
   ],
   "source": [
    "model=um.load_model(cfg,loading_path,device)\n",
    "prediction= model.forward(inp_p,inp_f,stat)\n",
    "\n",
    "torch.allclose(prediction,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
